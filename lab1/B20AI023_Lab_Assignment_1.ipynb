{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2"
      ],
      "metadata": {
        "id": "5-1CvOQoKzWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets"
      ],
      "metadata": {
        "id": "RMZ4uHjEE6f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "soFKZzO7J0aV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# One-hot encode the labels\n",
        "one_hot_y = np.zeros((y.shape[0], 3))\n",
        "one_hot_y[np.arange(y.shape[0]), y] = 1\n",
        "\n",
        "# y = np.eye(3)[y]\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, one_hot_y, test_size=0.2)\n",
        "\n",
        "# Define the number of neurons in each layer\n",
        "input_size = 4\n",
        "hidden1_size = 4\n",
        "hidden2_size = 5\n",
        "output_size = 3"
      ],
      "metadata": {
        "id": "kJfmM3VXtb1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Tanh Activation"
      ],
      "metadata": {
        "id": "kHkq4n87EVMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the weights randomly\n",
        "W1 = np.random.randn(input_size, hidden1_size)\n",
        "W2 = np.random.randn(hidden1_size, hidden2_size)\n",
        "W3 = np.random.randn(hidden2_size, output_size)\n",
        "\n",
        "# Define the learning rate\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Define the tanh function\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "# Define the tanh derivative function\n",
        "def tanh_derivative(x):\n",
        "    return 1 - (np.tanh(x)**2)\n",
        "\n",
        "# Define the cross-entropy loss function\n",
        "def cross_entropy(y_pred, y_true):\n",
        "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "# Perform backpropagation for 15 iterations\n",
        "for i in range(15):\n",
        "    # Forward pass\n",
        "    z1 = X_train @ W1\n",
        "    a1 = tanh(z1)\n",
        "    z2 = a1 @ W2\n",
        "    a2 = tanh(z2)\n",
        "    z3 = a2 @ W3\n",
        "    y_pred = tanh(z3)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = cross_entropy(y_pred, y_train)\n",
        "\n",
        "    # Backward pass\n",
        "    dz3 = y_pred - y_train\n",
        "    dw3 = a2.T @ dz3\n",
        "    dz2 = dz3 @ W3.T * tanh_derivative(a2)\n",
        "    dw2 = a1.T @ dz2\n",
        "    dz1 = dz2 @ W2.T * tanh_derivative(a1)\n",
        "    dw1 = X_train.T @ dz1\n",
        "\n",
        "    # Update the weights\n",
        "    W1 -= learning_rate * dw1\n",
        "    W2 -= learning_rate * dw2\n",
        "    W3 -= learning_rate * dw3\n",
        "\n",
        "    # Training Accuracy\n",
        "    predictions = np.argmax(y_pred, axis=1)\n",
        "    y_train_labels = np.argmax(y_train, axis=1)\n",
        "    accuracy = (predictions == y_train_labels).mean()\n",
        "\n",
        "    # Test Forward Pass\n",
        "    # Forward pass\n",
        "    z1 = X_test @ W1\n",
        "    a1 = tanh(z1)\n",
        "    z2 = a1 @ W2\n",
        "    a2 = tanh(z2)\n",
        "    z3 = a2 @ W3\n",
        "    y_pred_test = tanh(z3)\n",
        "\n",
        "    # Testing Accuracy\n",
        "    predictions_test = np.argmax(y_pred_test, axis=1)\n",
        "    y_test_labels = np.argmax(y_test, axis=1)\n",
        "    accuracy_test = (predictions_test == y_test_labels).mean()\n",
        "\n",
        "    print(f\"Epoch {i+1}:\")\n",
        "    print(\"Train accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "    print(\"Test accuracy: {:.2f}%\".format(accuracy_test * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riDsz6MhEU8T",
        "outputId": "aa817782-a915-4d52-d283-f0af3a5bfa1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n",
            "Train accuracy: 31.67%\n",
            "Test accuracy: 43.33%\n",
            "Epoch 2:\n",
            "Train accuracy: 49.17%\n",
            "Test accuracy: 20.00%\n",
            "Epoch 3:\n",
            "Train accuracy: 36.67%\n",
            "Test accuracy: 43.33%\n",
            "Epoch 4:\n",
            "Train accuracy: 30.83%\n",
            "Test accuracy: 20.00%\n",
            "Epoch 5:\n",
            "Train accuracy: 36.67%\n",
            "Test accuracy: 43.33%\n",
            "Epoch 6:\n",
            "Train accuracy: 30.83%\n",
            "Test accuracy: 20.00%\n",
            "Epoch 7:\n",
            "Train accuracy: 36.67%\n",
            "Test accuracy: 36.67%\n",
            "Epoch 8:\n",
            "Train accuracy: 32.50%\n",
            "Test accuracy: 43.33%\n",
            "Epoch 9:\n",
            "Train accuracy: 30.83%\n",
            "Test accuracy: 43.33%\n",
            "Epoch 10:\n",
            "Train accuracy: 30.83%\n",
            "Test accuracy: 36.67%\n",
            "Epoch 11:\n",
            "Train accuracy: 32.50%\n",
            "Test accuracy: 36.67%\n",
            "Epoch 12:\n",
            "Train accuracy: 32.50%\n",
            "Test accuracy: 20.00%\n",
            "Epoch 13:\n",
            "Train accuracy: 36.67%\n",
            "Test accuracy: 36.67%\n",
            "Epoch 14:\n",
            "Train accuracy: 32.50%\n",
            "Test accuracy: 20.00%\n",
            "Epoch 15:\n",
            "Train accuracy: 36.67%\n",
            "Test accuracy: 36.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-7f448e30324d>:19: RuntimeWarning: invalid value encountered in log\n",
            "  return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
            "<ipython-input-3-7f448e30324d>:19: RuntimeWarning: divide by zero encountered in log\n",
            "  return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
            "<ipython-input-3-7f448e30324d>:19: RuntimeWarning: invalid value encountered in multiply\n",
            "  return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1"
      ],
      "metadata": {
        "id": "9lvyH1DaKuYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets"
      ],
      "metadata": {
        "id": "5SzQbYr1Or7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pKwnaMLITBS",
        "outputId": "94517461-1d1e-4484-ad85-e46d6fcbf096"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2],\n",
              "       [5.4, 3.9, 1.7, 0.4],\n",
              "       [4.6, 3.4, 1.4, 0.3],\n",
              "       [5. , 3.4, 1.5, 0.2],\n",
              "       [4.4, 2.9, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.1],\n",
              "       [5.4, 3.7, 1.5, 0.2],\n",
              "       [4.8, 3.4, 1.6, 0.2],\n",
              "       [4.8, 3. , 1.4, 0.1],\n",
              "       [4.3, 3. , 1.1, 0.1],\n",
              "       [5.8, 4. , 1.2, 0.2],\n",
              "       [5.7, 4.4, 1.5, 0.4],\n",
              "       [5.4, 3.9, 1.3, 0.4],\n",
              "       [5.1, 3.5, 1.4, 0.3],\n",
              "       [5.7, 3.8, 1.7, 0.3],\n",
              "       [5.1, 3.8, 1.5, 0.3],\n",
              "       [5.4, 3.4, 1.7, 0.2],\n",
              "       [5.1, 3.7, 1.5, 0.4],\n",
              "       [4.6, 3.6, 1. , 0.2],\n",
              "       [5.1, 3.3, 1.7, 0.5],\n",
              "       [4.8, 3.4, 1.9, 0.2],\n",
              "       [5. , 3. , 1.6, 0.2],\n",
              "       [5. , 3.4, 1.6, 0.4],\n",
              "       [5.2, 3.5, 1.5, 0.2],\n",
              "       [5.2, 3.4, 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.6, 0.2],\n",
              "       [4.8, 3.1, 1.6, 0.2],\n",
              "       [5.4, 3.4, 1.5, 0.4],\n",
              "       [5.2, 4.1, 1.5, 0.1],\n",
              "       [5.5, 4.2, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.2, 1.2, 0.2],\n",
              "       [5.5, 3.5, 1.3, 0.2],\n",
              "       [4.9, 3.6, 1.4, 0.1],\n",
              "       [4.4, 3. , 1.3, 0.2],\n",
              "       [5.1, 3.4, 1.5, 0.2],\n",
              "       [5. , 3.5, 1.3, 0.3],\n",
              "       [4.5, 2.3, 1.3, 0.3],\n",
              "       [4.4, 3.2, 1.3, 0.2],\n",
              "       [5. , 3.5, 1.6, 0.6],\n",
              "       [5.1, 3.8, 1.9, 0.4],\n",
              "       [4.8, 3. , 1.4, 0.3],\n",
              "       [5.1, 3.8, 1.6, 0.2],\n",
              "       [4.6, 3.2, 1.4, 0.2],\n",
              "       [5.3, 3.7, 1.5, 0.2],\n",
              "       [5. , 3.3, 1.4, 0.2],\n",
              "       [7. , 3.2, 4.7, 1.4],\n",
              "       [6.4, 3.2, 4.5, 1.5],\n",
              "       [6.9, 3.1, 4.9, 1.5],\n",
              "       [5.5, 2.3, 4. , 1.3],\n",
              "       [6.5, 2.8, 4.6, 1.5],\n",
              "       [5.7, 2.8, 4.5, 1.3],\n",
              "       [6.3, 3.3, 4.7, 1.6],\n",
              "       [4.9, 2.4, 3.3, 1. ],\n",
              "       [6.6, 2.9, 4.6, 1.3],\n",
              "       [5.2, 2.7, 3.9, 1.4],\n",
              "       [5. , 2. , 3.5, 1. ],\n",
              "       [5.9, 3. , 4.2, 1.5],\n",
              "       [6. , 2.2, 4. , 1. ],\n",
              "       [6.1, 2.9, 4.7, 1.4],\n",
              "       [5.6, 2.9, 3.6, 1.3],\n",
              "       [6.7, 3.1, 4.4, 1.4],\n",
              "       [5.6, 3. , 4.5, 1.5],\n",
              "       [5.8, 2.7, 4.1, 1. ],\n",
              "       [6.2, 2.2, 4.5, 1.5],\n",
              "       [5.6, 2.5, 3.9, 1.1],\n",
              "       [5.9, 3.2, 4.8, 1.8],\n",
              "       [6.1, 2.8, 4. , 1.3],\n",
              "       [6.3, 2.5, 4.9, 1.5],\n",
              "       [6.1, 2.8, 4.7, 1.2],\n",
              "       [6.4, 2.9, 4.3, 1.3],\n",
              "       [6.6, 3. , 4.4, 1.4],\n",
              "       [6.8, 2.8, 4.8, 1.4],\n",
              "       [6.7, 3. , 5. , 1.7],\n",
              "       [6. , 2.9, 4.5, 1.5],\n",
              "       [5.7, 2.6, 3.5, 1. ],\n",
              "       [5.5, 2.4, 3.8, 1.1],\n",
              "       [5.5, 2.4, 3.7, 1. ],\n",
              "       [5.8, 2.7, 3.9, 1.2],\n",
              "       [6. , 2.7, 5.1, 1.6],\n",
              "       [5.4, 3. , 4.5, 1.5],\n",
              "       [6. , 3.4, 4.5, 1.6],\n",
              "       [6.7, 3.1, 4.7, 1.5],\n",
              "       [6.3, 2.3, 4.4, 1.3],\n",
              "       [5.6, 3. , 4.1, 1.3],\n",
              "       [5.5, 2.5, 4. , 1.3],\n",
              "       [5.5, 2.6, 4.4, 1.2],\n",
              "       [6.1, 3. , 4.6, 1.4],\n",
              "       [5.8, 2.6, 4. , 1.2],\n",
              "       [5. , 2.3, 3.3, 1. ],\n",
              "       [5.6, 2.7, 4.2, 1.3],\n",
              "       [5.7, 3. , 4.2, 1.2],\n",
              "       [5.7, 2.9, 4.2, 1.3],\n",
              "       [6.2, 2.9, 4.3, 1.3],\n",
              "       [5.1, 2.5, 3. , 1.1],\n",
              "       [5.7, 2.8, 4.1, 1.3],\n",
              "       [6.3, 3.3, 6. , 2.5],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [7.1, 3. , 5.9, 2.1],\n",
              "       [6.3, 2.9, 5.6, 1.8],\n",
              "       [6.5, 3. , 5.8, 2.2],\n",
              "       [7.6, 3. , 6.6, 2.1],\n",
              "       [4.9, 2.5, 4.5, 1.7],\n",
              "       [7.3, 2.9, 6.3, 1.8],\n",
              "       [6.7, 2.5, 5.8, 1.8],\n",
              "       [7.2, 3.6, 6.1, 2.5],\n",
              "       [6.5, 3.2, 5.1, 2. ],\n",
              "       [6.4, 2.7, 5.3, 1.9],\n",
              "       [6.8, 3. , 5.5, 2.1],\n",
              "       [5.7, 2.5, 5. , 2. ],\n",
              "       [5.8, 2.8, 5.1, 2.4],\n",
              "       [6.4, 3.2, 5.3, 2.3],\n",
              "       [6.5, 3. , 5.5, 1.8],\n",
              "       [7.7, 3.8, 6.7, 2.2],\n",
              "       [7.7, 2.6, 6.9, 2.3],\n",
              "       [6. , 2.2, 5. , 1.5],\n",
              "       [6.9, 3.2, 5.7, 2.3],\n",
              "       [5.6, 2.8, 4.9, 2. ],\n",
              "       [7.7, 2.8, 6.7, 2. ],\n",
              "       [6.3, 2.7, 4.9, 1.8],\n",
              "       [6.7, 3.3, 5.7, 2.1],\n",
              "       [7.2, 3.2, 6. , 1.8],\n",
              "       [6.2, 2.8, 4.8, 1.8],\n",
              "       [6.1, 3. , 4.9, 1.8],\n",
              "       [6.4, 2.8, 5.6, 2.1],\n",
              "       [7.2, 3. , 5.8, 1.6],\n",
              "       [7.4, 2.8, 6.1, 1.9],\n",
              "       [7.9, 3.8, 6.4, 2. ],\n",
              "       [6.4, 2.8, 5.6, 2.2],\n",
              "       [6.3, 2.8, 5.1, 1.5],\n",
              "       [6.1, 2.6, 5.6, 1.4],\n",
              "       [7.7, 3. , 6.1, 2.3],\n",
              "       [6.3, 3.4, 5.6, 2.4],\n",
              "       [6.4, 3.1, 5.5, 1.8],\n",
              "       [6. , 3. , 4.8, 1.8],\n",
              "       [6.9, 3.1, 5.4, 2.1],\n",
              "       [6.7, 3.1, 5.6, 2.4],\n",
              "       [6.9, 3.1, 5.1, 2.3],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [6.8, 3.2, 5.9, 2.3],\n",
              "       [6.7, 3.3, 5.7, 2.5],\n",
              "       [6.7, 3. , 5.2, 2.3],\n",
              "       [6.3, 2.5, 5. , 1.9],\n",
              "       [6.5, 3. , 5.2, 2. ],\n",
              "       [6.2, 3.4, 5.4, 2.3],\n",
              "       [5.9, 3. , 5.1, 1.8]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "\n",
        "\n",
        "# importing and loading the IRIS dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data # features\n",
        "y = iris.target # target\n",
        "\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXjYspIBNa_P",
        "outputId": "4870085f-2165-44ec-90d6-7a148223a852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling the data for improving upon the accuracy\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG_pa9wiNf79",
        "outputId": "460f141d-dd24-4eb9-a30f-42fc41ef894f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.22222222, 0.625     , 0.06779661, 0.04166667],\n",
              "       [0.16666667, 0.41666667, 0.06779661, 0.04166667],\n",
              "       [0.11111111, 0.5       , 0.05084746, 0.04166667],\n",
              "       [0.08333333, 0.45833333, 0.08474576, 0.04166667],\n",
              "       [0.19444444, 0.66666667, 0.06779661, 0.04166667],\n",
              "       [0.30555556, 0.79166667, 0.11864407, 0.125     ],\n",
              "       [0.08333333, 0.58333333, 0.06779661, 0.08333333],\n",
              "       [0.19444444, 0.58333333, 0.08474576, 0.04166667],\n",
              "       [0.02777778, 0.375     , 0.06779661, 0.04166667],\n",
              "       [0.16666667, 0.45833333, 0.08474576, 0.        ],\n",
              "       [0.30555556, 0.70833333, 0.08474576, 0.04166667],\n",
              "       [0.13888889, 0.58333333, 0.10169492, 0.04166667],\n",
              "       [0.13888889, 0.41666667, 0.06779661, 0.        ],\n",
              "       [0.        , 0.41666667, 0.01694915, 0.        ],\n",
              "       [0.41666667, 0.83333333, 0.03389831, 0.04166667],\n",
              "       [0.38888889, 1.        , 0.08474576, 0.125     ],\n",
              "       [0.30555556, 0.79166667, 0.05084746, 0.125     ],\n",
              "       [0.22222222, 0.625     , 0.06779661, 0.08333333],\n",
              "       [0.38888889, 0.75      , 0.11864407, 0.08333333],\n",
              "       [0.22222222, 0.75      , 0.08474576, 0.08333333],\n",
              "       [0.30555556, 0.58333333, 0.11864407, 0.04166667],\n",
              "       [0.22222222, 0.70833333, 0.08474576, 0.125     ],\n",
              "       [0.08333333, 0.66666667, 0.        , 0.04166667],\n",
              "       [0.22222222, 0.54166667, 0.11864407, 0.16666667],\n",
              "       [0.13888889, 0.58333333, 0.15254237, 0.04166667],\n",
              "       [0.19444444, 0.41666667, 0.10169492, 0.04166667],\n",
              "       [0.19444444, 0.58333333, 0.10169492, 0.125     ],\n",
              "       [0.25      , 0.625     , 0.08474576, 0.04166667],\n",
              "       [0.25      , 0.58333333, 0.06779661, 0.04166667],\n",
              "       [0.11111111, 0.5       , 0.10169492, 0.04166667],\n",
              "       [0.13888889, 0.45833333, 0.10169492, 0.04166667],\n",
              "       [0.30555556, 0.58333333, 0.08474576, 0.125     ],\n",
              "       [0.25      , 0.875     , 0.08474576, 0.        ],\n",
              "       [0.33333333, 0.91666667, 0.06779661, 0.04166667],\n",
              "       [0.16666667, 0.45833333, 0.08474576, 0.04166667],\n",
              "       [0.19444444, 0.5       , 0.03389831, 0.04166667],\n",
              "       [0.33333333, 0.625     , 0.05084746, 0.04166667],\n",
              "       [0.16666667, 0.66666667, 0.06779661, 0.        ],\n",
              "       [0.02777778, 0.41666667, 0.05084746, 0.04166667],\n",
              "       [0.22222222, 0.58333333, 0.08474576, 0.04166667],\n",
              "       [0.19444444, 0.625     , 0.05084746, 0.08333333],\n",
              "       [0.05555556, 0.125     , 0.05084746, 0.08333333],\n",
              "       [0.02777778, 0.5       , 0.05084746, 0.04166667],\n",
              "       [0.19444444, 0.625     , 0.10169492, 0.20833333],\n",
              "       [0.22222222, 0.75      , 0.15254237, 0.125     ],\n",
              "       [0.13888889, 0.41666667, 0.06779661, 0.08333333],\n",
              "       [0.22222222, 0.75      , 0.10169492, 0.04166667],\n",
              "       [0.08333333, 0.5       , 0.06779661, 0.04166667],\n",
              "       [0.27777778, 0.70833333, 0.08474576, 0.04166667],\n",
              "       [0.19444444, 0.54166667, 0.06779661, 0.04166667],\n",
              "       [0.75      , 0.5       , 0.62711864, 0.54166667],\n",
              "       [0.58333333, 0.5       , 0.59322034, 0.58333333],\n",
              "       [0.72222222, 0.45833333, 0.66101695, 0.58333333],\n",
              "       [0.33333333, 0.125     , 0.50847458, 0.5       ],\n",
              "       [0.61111111, 0.33333333, 0.61016949, 0.58333333],\n",
              "       [0.38888889, 0.33333333, 0.59322034, 0.5       ],\n",
              "       [0.55555556, 0.54166667, 0.62711864, 0.625     ],\n",
              "       [0.16666667, 0.16666667, 0.38983051, 0.375     ],\n",
              "       [0.63888889, 0.375     , 0.61016949, 0.5       ],\n",
              "       [0.25      , 0.29166667, 0.49152542, 0.54166667],\n",
              "       [0.19444444, 0.        , 0.42372881, 0.375     ],\n",
              "       [0.44444444, 0.41666667, 0.54237288, 0.58333333],\n",
              "       [0.47222222, 0.08333333, 0.50847458, 0.375     ],\n",
              "       [0.5       , 0.375     , 0.62711864, 0.54166667],\n",
              "       [0.36111111, 0.375     , 0.44067797, 0.5       ],\n",
              "       [0.66666667, 0.45833333, 0.57627119, 0.54166667],\n",
              "       [0.36111111, 0.41666667, 0.59322034, 0.58333333],\n",
              "       [0.41666667, 0.29166667, 0.52542373, 0.375     ],\n",
              "       [0.52777778, 0.08333333, 0.59322034, 0.58333333],\n",
              "       [0.36111111, 0.20833333, 0.49152542, 0.41666667],\n",
              "       [0.44444444, 0.5       , 0.6440678 , 0.70833333],\n",
              "       [0.5       , 0.33333333, 0.50847458, 0.5       ],\n",
              "       [0.55555556, 0.20833333, 0.66101695, 0.58333333],\n",
              "       [0.5       , 0.33333333, 0.62711864, 0.45833333],\n",
              "       [0.58333333, 0.375     , 0.55932203, 0.5       ],\n",
              "       [0.63888889, 0.41666667, 0.57627119, 0.54166667],\n",
              "       [0.69444444, 0.33333333, 0.6440678 , 0.54166667],\n",
              "       [0.66666667, 0.41666667, 0.6779661 , 0.66666667],\n",
              "       [0.47222222, 0.375     , 0.59322034, 0.58333333],\n",
              "       [0.38888889, 0.25      , 0.42372881, 0.375     ],\n",
              "       [0.33333333, 0.16666667, 0.47457627, 0.41666667],\n",
              "       [0.33333333, 0.16666667, 0.45762712, 0.375     ],\n",
              "       [0.41666667, 0.29166667, 0.49152542, 0.45833333],\n",
              "       [0.47222222, 0.29166667, 0.69491525, 0.625     ],\n",
              "       [0.30555556, 0.41666667, 0.59322034, 0.58333333],\n",
              "       [0.47222222, 0.58333333, 0.59322034, 0.625     ],\n",
              "       [0.66666667, 0.45833333, 0.62711864, 0.58333333],\n",
              "       [0.55555556, 0.125     , 0.57627119, 0.5       ],\n",
              "       [0.36111111, 0.41666667, 0.52542373, 0.5       ],\n",
              "       [0.33333333, 0.20833333, 0.50847458, 0.5       ],\n",
              "       [0.33333333, 0.25      , 0.57627119, 0.45833333],\n",
              "       [0.5       , 0.41666667, 0.61016949, 0.54166667],\n",
              "       [0.41666667, 0.25      , 0.50847458, 0.45833333],\n",
              "       [0.19444444, 0.125     , 0.38983051, 0.375     ],\n",
              "       [0.36111111, 0.29166667, 0.54237288, 0.5       ],\n",
              "       [0.38888889, 0.41666667, 0.54237288, 0.45833333],\n",
              "       [0.38888889, 0.375     , 0.54237288, 0.5       ],\n",
              "       [0.52777778, 0.375     , 0.55932203, 0.5       ],\n",
              "       [0.22222222, 0.20833333, 0.33898305, 0.41666667],\n",
              "       [0.38888889, 0.33333333, 0.52542373, 0.5       ],\n",
              "       [0.55555556, 0.54166667, 0.84745763, 1.        ],\n",
              "       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n",
              "       [0.77777778, 0.41666667, 0.83050847, 0.83333333],\n",
              "       [0.55555556, 0.375     , 0.77966102, 0.70833333],\n",
              "       [0.61111111, 0.41666667, 0.81355932, 0.875     ],\n",
              "       [0.91666667, 0.41666667, 0.94915254, 0.83333333],\n",
              "       [0.16666667, 0.20833333, 0.59322034, 0.66666667],\n",
              "       [0.83333333, 0.375     , 0.89830508, 0.70833333],\n",
              "       [0.66666667, 0.20833333, 0.81355932, 0.70833333],\n",
              "       [0.80555556, 0.66666667, 0.86440678, 1.        ],\n",
              "       [0.61111111, 0.5       , 0.69491525, 0.79166667],\n",
              "       [0.58333333, 0.29166667, 0.72881356, 0.75      ],\n",
              "       [0.69444444, 0.41666667, 0.76271186, 0.83333333],\n",
              "       [0.38888889, 0.20833333, 0.6779661 , 0.79166667],\n",
              "       [0.41666667, 0.33333333, 0.69491525, 0.95833333],\n",
              "       [0.58333333, 0.5       , 0.72881356, 0.91666667],\n",
              "       [0.61111111, 0.41666667, 0.76271186, 0.70833333],\n",
              "       [0.94444444, 0.75      , 0.96610169, 0.875     ],\n",
              "       [0.94444444, 0.25      , 1.        , 0.91666667],\n",
              "       [0.47222222, 0.08333333, 0.6779661 , 0.58333333],\n",
              "       [0.72222222, 0.5       , 0.79661017, 0.91666667],\n",
              "       [0.36111111, 0.33333333, 0.66101695, 0.79166667],\n",
              "       [0.94444444, 0.33333333, 0.96610169, 0.79166667],\n",
              "       [0.55555556, 0.29166667, 0.66101695, 0.70833333],\n",
              "       [0.66666667, 0.54166667, 0.79661017, 0.83333333],\n",
              "       [0.80555556, 0.5       , 0.84745763, 0.70833333],\n",
              "       [0.52777778, 0.33333333, 0.6440678 , 0.70833333],\n",
              "       [0.5       , 0.41666667, 0.66101695, 0.70833333],\n",
              "       [0.58333333, 0.33333333, 0.77966102, 0.83333333],\n",
              "       [0.80555556, 0.41666667, 0.81355932, 0.625     ],\n",
              "       [0.86111111, 0.33333333, 0.86440678, 0.75      ],\n",
              "       [1.        , 0.75      , 0.91525424, 0.79166667],\n",
              "       [0.58333333, 0.33333333, 0.77966102, 0.875     ],\n",
              "       [0.55555556, 0.33333333, 0.69491525, 0.58333333],\n",
              "       [0.5       , 0.25      , 0.77966102, 0.54166667],\n",
              "       [0.94444444, 0.41666667, 0.86440678, 0.91666667],\n",
              "       [0.55555556, 0.58333333, 0.77966102, 0.95833333],\n",
              "       [0.58333333, 0.45833333, 0.76271186, 0.70833333],\n",
              "       [0.47222222, 0.41666667, 0.6440678 , 0.70833333],\n",
              "       [0.72222222, 0.45833333, 0.74576271, 0.83333333],\n",
              "       [0.66666667, 0.45833333, 0.77966102, 0.95833333],\n",
              "       [0.72222222, 0.45833333, 0.69491525, 0.91666667],\n",
              "       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n",
              "       [0.69444444, 0.5       , 0.83050847, 0.91666667],\n",
              "       [0.66666667, 0.54166667, 0.79661017, 1.        ],\n",
              "       [0.66666667, 0.41666667, 0.71186441, 0.91666667],\n",
              "       [0.55555556, 0.20833333, 0.6779661 , 0.75      ],\n",
              "       [0.61111111, 0.41666667, 0.71186441, 0.79166667],\n",
              "       [0.52777778, 0.58333333, 0.74576271, 0.91666667],\n",
              "       [0.44444444, 0.41666667, 0.69491525, 0.70833333]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# doing one hot encoding\n",
        "one_hot_y = np.zeros((y.shape[0], 3))\n",
        "one_hot_y[np.arange(y.shape[0]), y] = 1\n",
        "\n",
        "# tts\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, one_hot_y, test_size=0.2)"
      ],
      "metadata": {
        "id": "d3xmDMimNkKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assigning random weight and bias\n",
        "weights = np.random.rand(4, 3)\n",
        "bias = np.random.rand(1, 3)\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "# backpropagation for 2 iterations\n",
        "for _ in range(2):\n",
        "    # forward pass\n",
        "    output = np.dot(X_train, weights) + bias\n",
        "    output = np.maximum(0, output)  # ReLU activation function\n",
        "    output = np.exp(output) / np.sum(np.exp(output), axis=1, keepdims=True)  # Softmax function\n",
        "    \n",
        "    # Compute the loss (cross-entropy)\n",
        "    loss = -np.mean(y_train * np.log(output))\n",
        "    print(\"Loss:\", loss)\n",
        "\n",
        "    # Compute the gradients\n",
        "    error = output - y_train\n",
        "    weights_grad = np.dot(X_train.T, error)\n",
        "    bias_grad = np.sum(error, axis=0, keepdims=True)\n",
        "\n",
        "    # Update the trainable parameters\n",
        "    weights -= learning_rate * weights_grad\n",
        "    bias -= learning_rate * bias_grad\n",
        "\n",
        "# testing the model\n",
        "output = np.dot(X_test, weights) + bias\n",
        "output = np.maximum(0, output)  # ReLU activation function\n",
        "output = np.exp(output) / np.sum(np.exp(output), axis=1, keepdims=True)  # Softmax function for output layer\n",
        "y_pred = np.argmax(output, axis=1)\n",
        "\n",
        "# Compute the accuracy of the model\n",
        "\n",
        "accuracy = np.mean(y_pred == np.argmax(y_test, axis=1))\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHfm96R8NWzw",
        "outputId": "eabeb644-0855-475b-f28d-8dfd82c8d66a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.3561151506391446\n",
            "Loss: 0.28916319563704523\n",
            "Accuracy: 0.6666666666666666\n"
          ]
        }
      ]
    }
  ]
}